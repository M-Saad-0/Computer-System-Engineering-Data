{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 7: Ensemble Methods for Student Pass/Fail Prediction\n",
    "\n",
    "**Objective:** To build, compare, and evaluate ensemble machine learning models (Random Forest, AdaBoost, Gradient Boosting) for predicting whether a student will pass or fail based on academic and behavioral features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optional: Improve plot aesthetics using seaborn\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_style(\"whitegrid\")\n",
    "except ImportError:\n",
    "    pass # Keep default matplotlib style if seaborn is not installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "Since no specific dataset is provided, we will create a synthetic dataset representative of student academic and behavioral features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "np.random.seed(42) # for reproducibility\n",
    "n_students = 1000\n",
    "\n",
    "data = {\n",
    "    'StudyHours': np.random.uniform(1, 20, n_students),\n",
    "    'PreviousGPA': np.random.uniform(1.5, 4.0, n_students),\n",
    "    'AttendancePercentage': np.random.uniform(50, 100, n_students),\n",
    "    'AssignmentsCompleted': np.random.randint(0, 11, n_students), # 0 to 10 assignments\n",
    "    'EngagementLevel': np.random.choice(['Low', 'Medium', 'High'], n_students, p=[0.3, 0.5, 0.2]),\n",
    "    'HasTutor': np.random.choice([0, 1], n_students, p=[0.7, 0.3]) # 0: No, 1: Yes\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a synthetic target variable 'Pass' (1 for Pass, 0 for Fail)\n",
    "# Probability of passing increases with better metrics\n",
    "prob_pass = (df['StudyHours']/20 + \n",
    "             df['PreviousGPA']/4 + \n",
    "             df['AttendancePercentage']/100 + \n",
    "             df['AssignmentsCompleted']/10 + \n",
    "             df['EngagementLevel'].map({'Low': 0.1, 'Medium': 0.5, 'High': 0.9}) + \n",
    "             df['HasTutor']*0.1) / 5 # Normalize probability factor\n",
    "\n",
    "# Add some noise\n",
    "prob_pass = np.clip(prob_pass + np.random.normal(0, 0.15, n_students), 0, 1)\n",
    "\n",
    "# Determine pass/fail based on probability threshold (e.g., 0.5)\n",
    "df['Pass'] = (prob_pass > 0.5).astype(int)\n",
    "\n",
    "print(\"Dataset Head:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(df['Pass'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Preprocessing\n",
    "\n",
    "We need to:\n",
    "1.  Identify numerical and categorical features.\n",
    "2.  Encode categorical features (e.g., using One-Hot Encoding).\n",
    "3.  Potentially scale numerical features (though tree-based models are less sensitive to scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop('Pass', axis=1)\n",
    "y = df['Pass']\n",
    "\n",
    "# Identify feature types\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"Numerical Features: {numerical_features}\")\n",
    "print(f\"Categorical Features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "# For tree models, scaling isn't strictly necessary, but we'll include it for completeness\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Create a column transformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing (fit_transform on the whole dataset for demonstration, \n",
    "# but typically fit on training and transform both train/test)\n",
    "# We will integrate this into pipelines later for proper train/test split handling\n",
    "# X_processed = preprocessor.fit_transform(X)\n",
    "# print(f\"\\nShape of processed features: {X_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split\n",
    "\n",
    "Split the data into training and testing sets to evaluate model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Testing set shape: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Building and Training\n",
    "\n",
    "We will create pipelines that include the preprocessing steps and the respective ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest --- \n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', RandomForestClassifier(random_state=42, n_estimators=100))])\n",
    "\n",
    "# --- AdaBoost --- \n",
    "ab_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             ('classifier', AdaBoostClassifier(random_state=42, n_estimators=50))]) \n",
    "# Note: AdaBoost often uses DecisionTreeClassifier(max_depth=1) as base estimator by default\n",
    "\n",
    "# --- Gradient Boosting --- \n",
    "gb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', GradientBoostingClassifier(random_state=42, n_estimators=100))])\n",
    "\n",
    "# Train the models\n",
    "print(\"Training Random Forest...\")\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "print(\"Training AdaBoost...\")\n",
    "ab_pipeline.fit(X_train, y_train)\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nAll models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate each model on the test set using standard classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "y_pred_ab = ab_pipeline.predict(X_test)\n",
    "y_pred_gb = gb_pipeline.predict(X_test)\n",
    "\n",
    "# Get prediction probabilities for AUC\n",
    "y_prob_rf = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_prob_ab = ab_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_prob_gb = gb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "\n",
    "print(\"--- Random Forest Evaluation ---\")\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "print(f\"Accuracy: {acc_rf:.4f}\")\n",
    "print(f\"ROC AUC: {auc_rf:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "results['Random Forest'] = {'Accuracy': acc_rf, 'AUC': auc_rf}\n",
    "\n",
    "print(\"\\n--- AdaBoost Evaluation ---\")\n",
    "acc_ab = accuracy_score(y_test, y_pred_ab)\n",
    "auc_ab = roc_auc_score(y_test, y_prob_ab)\n",
    "print(f\"Accuracy: {acc_ab:.4f}\")\n",
    "print(f\"ROC AUC: {auc_ab:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ab))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_ab))\n",
    "results['AdaBoost'] = {'Accuracy': acc_ab, 'AUC': auc_ab}\n",
    "\n",
    "print(\"\\n--- Gradient Boosting Evaluation ---\")\n",
    "acc_gb = accuracy_score(y_test, y_pred_gb)\n",
    "auc_gb = roc_auc_score(y_test, y_prob_gb)\n",
    "print(f\"Accuracy: {acc_gb:.4f}\")\n",
    "print(f\"ROC AUC: {auc_gb:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gb))\n",
    "results['Gradient Boosting'] = {'Accuracy': acc_gb, 'AUC': auc_gb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison\n",
    "\n",
    "Summarize the performance of the three ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model Performance Summary ---\")\n",
    "results_df = pd.DataFrame(results).T # Transpose for better readability\n",
    "print(results_df)\n",
    "\n",
    "# Find the best model based on a chosen metric (e.g., ROC AUC)\n",
    "best_model_auc = results_df['AUC'].idxmax()\n",
    "best_model_acc = results_df['Accuracy'].idxmax()\n",
    "\n",
    "print(f\"\\nBest model based on ROC AUC: {best_model_auc} (AUC: {results_df.loc[best_model_auc, 'AUC']:.4f})\")\n",
    "print(f\"Best model based on Accuracy: {best_model_acc} (Accuracy: {results_df.loc[best_model_acc, 'Accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Performance Bar Chart --- \n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy Plot\n",
    "results_df['Accuracy'].plot(kind='bar', ax=ax[0], color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax[0].set_title('Model Accuracy Comparison')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Model')\n",
    "ax[0].tick_params(axis='x', rotation=0)\n",
    "ax[0].set_ylim(bottom=max(0, results_df['Accuracy'].min() - 0.05), top=min(1.0, results_df['Accuracy'].max() + 0.05)) # Adjust y-lim dynamically\n",
    "\n",
    "# AUC Plot\n",
    "results_df['AUC'].plot(kind='bar', ax=ax[1], color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax[1].set_title('Model ROC AUC Comparison')\n",
    "ax[1].set_ylabel('ROC AUC Score')\n",
    "ax[1].set_xlabel('Model')\n",
    "ax[1].tick_params(axis='x', rotation=0)\n",
    "ax[1].set_ylim(bottom=max(0, results_df['AUC'].min() - 0.05), top=min(1.0, results_df['AUC'].max() + 0.05)) # Adjust y-lim dynamically\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- ROC Curve Plot ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Random Forest\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.4f})')\n",
    "\n",
    "# AdaBoost\n",
    "fpr_ab, tpr_ab, _ = roc_curve(y_test, y_prob_ab)\n",
    "plt.plot(fpr_ab, tpr_ab, label=f'AdaBoost (AUC = {auc_ab:.4f})')\n",
    "\n",
    "# Gradient Boosting\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_prob_gb)\n",
    "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {auc_gb:.4f})')\n",
    "\n",
    "# Plotting the diagonal line (random guessing)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook demonstrated the process of building, training, and evaluating three common ensemble classifiers (Random Forest, AdaBoost, Gradient Boosting) for a binary classification task (student pass/fail prediction) using a synthetic dataset.\n",
    "\n",
    "Based on the evaluation metrics (specifically Accuracy and ROC AUC) and the visualizations above, we compared their performance on the unseen test data.\n",
    "\n",
    "* **Random Forest** typically performs well out-of-the-box and is robust to overfitting with enough trees.\n",
    "* **AdaBoost** focuses on misclassified samples, which can be powerful but sometimes sensitive to noisy data or outliers.\n",
    "* **Gradient Boosting** builds trees sequentially, correcting errors from previous trees, often leading to high accuracy but potentially requiring more careful tuning.\n",
    "\n",
    "The 'best' model depends on the specific dataset characteristics and the chosen evaluation metric. In this synthetic example, [mention the best performing model based on the output, e.g., Gradient Boosting or Random Forest] showed slightly better performance according to [mention metric, e.g., ROC AUC]. The visualizations provide a clear comparison of these metrics.\n",
    "\n",
    "**Further Steps:**\n",
    "* Hyperparameter tuning (e.g., using GridSearchCV or RandomizedSearchCV) for each model could further optimize performance.\n",
    "* Feature importance analysis could reveal which academic/behavioral factors are most predictive (especially useful with Random Forest and Gradient Boosting).\n",
    "* Trying other ensemble techniques (e.g., XGBoost, LightGBM, CatBoost) could yield different results.\n",
    "* Using a real-world dataset would provide more meaningful insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

