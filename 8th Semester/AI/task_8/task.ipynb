{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting California House Prices using an Artificial Neural Network (ANN)\n",
    "\n",
    "This notebook demonstrates how to build and train an Artificial Neural Network (ANN) using TensorFlow/Keras to predict median house prices in California based on the California Housing dataset available in scikit-learn.\n",
    "\n",
    "**Objective:**\n",
    "Use an ANN model to predict median house prices in California based on input features like population, income, house age, and location-based statistics from the California Housing dataset.\n",
    "\n",
    "**Model Requirements:**\n",
    "- 1 input layer (8 features)\n",
    "- At least 2 hidden layers (e.g., 64 and 32 neurons)\n",
    "- 1 output neuron\n",
    "- ReLU activation function for hidden layers\n",
    "- MSE as the loss function\n",
    "- Adam optimizer\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- R² score\n",
    "- Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Often useful for better-looking plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the California Housing dataset using `Workspace_california_housing` from `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Create a pandas DataFrame for features (X) and a Series for the target (y)\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = pd.Series(housing.target, name='median_house_value')\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"Feature names:\", housing.feature_names)\n",
    "# Display the first few rows of the features and target\n",
    "print(\"\\nFeatures (first 5 rows):\\n\", X.head())\n",
    "print(\"\\nTarget (first 5 values):\\n\", y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks generally perform better when input features are scaled. We will split the data into training and testing sets and then use `StandardScaler` to standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nData split into training and testing sets.\")\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Testing target shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the same scaler (important!)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeatures scaled successfully.\")\n",
    "print(\"Scaled training features shape:\", X_train_scaled.shape)\n",
    "print(\"Scaled testing features shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the ANN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a Sequential Keras model with the specified architecture:\n",
    "- A `Dense` layer with 64 neurons and ReLU activation, serving as the first hidden layer and implicitly the input layer (by setting `input_shape`). The `input_shape` is the number of features in our scaled data.\n",
    "- A second `Dense` layer with 32 neurons and ReLU activation.\n",
    "- An output `Dense` layer with 1 neuron for predicting the continuous house price. No activation function is typically used in the output layer for regression tasks.\n",
    "\n",
    "The model is compiled using the Adam optimizer and Mean Squared Error (MSE) as the loss function. We also track MSE as a metric during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Define the Sequential model\n",
    "model = Sequential([\n",
    "    # Input layer and First Hidden layer\n",
    "    Dense(64, activation='relu', input_shape=(n_features,)),\n",
    "    # Second Hidden layer\n",
    "    Dense(32, activation='relu'),\n",
    "    # Output layer for regression\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', # Adam optimizer\n",
    "              loss='mse',       # Mean Squared Error loss function\n",
    "              metrics=['mse'])  # Track MSE during training\n",
    "\n",
    "# Print the model summary to see the architecture\n",
    "print(\"ANN Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the scaled training data (`X_train_scaled`) and the corresponding target values (`y_train`). We specify the number of epochs (iterations over the dataset) and the batch size (number of samples per gradient update). A validation split is used to monitor the model's performance on a portion of the training data that is not used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# We use a validation split to monitor performance on unseen data during training\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=100,         # Number of training epochs\n",
    "                    batch_size=64,      # Number of samples per gradient update\n",
    "                    validation_split=0.2, # Split 20% of training data for validation\n",
    "                    verbose=1)          # Show training progress\n",
    "print(\"Model training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we evaluate the model's performance on the unseen test dataset (`X_test_scaled`, `y_test`) using the specified metrics: MSE and R² score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "loss, mse = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f'Test Mean Squared Error (MSE): {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate R-squared (R²) score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Test R^2 Score: {r2:.4f}')"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Performance and Training History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the training history and the model's predictions versus the actual values can provide further insights into its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Training and Validation Loss Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Actual vs. Predicted Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot comparing the actual house prices from the test set (`y_test`) against the prices predicted by the model (`y_pred`). Points close to the diagonal line indicate accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3) # Use alpha to show density of points\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2) # Red dashed line for perfect prediction\n",
    "plt.xlabel('Actual Median House Value ($100k)')\n",
    "plt.ylabel('Predicted Median House Value ($100k)')\n",
    "plt.title('Actual vs. Predicted House Prices (Test Set)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Distribution of Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals are the differences between the actual values and the predicted values ($ Actual - Predicted $). Plotting their distribution helps understand the nature of the model's errors. Ideally, residuals should be randomly distributed around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "# y_pred comes as a 2D array (n_samples, 1), flatten it for residual calculation with 1D y_test\n",
    "residuals = y_test - y_pred.flatten()\n",
    "\n",
    "# Plot histogram of residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, bins=50, kde=True) # Use seaborn for a nice histogram with KDE\n",
    "plt.title('Distribution of Residuals (Test Set)')\n",
    "plt.xlabel('Residual (Actual - Predicted)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Further Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook successfully implemented and trained an Artificial Neural Network to predict median house prices in California using the specified architecture, loss function, and optimizer. The model's performance was evaluated using MSE and R² on the test set, and we visualized the training progress and prediction quality.\n",
    "\n",
    "The plots help confirm the model's performance: the loss decreases during training, the actual vs. predicted plot shows a reasonable linear relationship (though with scatter), and the residuals plot indicates the distribution of errors.\n",
    "\n",
    "Further steps to potentially improve the model's performance include:\n",
    "- **Hyperparameter Tuning:** Experiment with different numbers of layers, neurons per layer, activation functions, learning rates, epochs, and batch sizes.\n",
    "- **Regularization:** Add techniques like Dropout or L1/L2 regularization to prevent overfitting, especially if the validation loss plot shows signs of divergence.\n",
    "- **Cross-Validation:** Use k-fold cross-validation for a more robust evaluation of the model's performance.\n",
    "- **Feature Engineering:** Create new features from existing ones that might be more informative.\n",
    "- **Different Architectures:** Explore more complex ANN architectures or other types of models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}